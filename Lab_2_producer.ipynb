{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "disciplinary-field",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web UI: http://10.128.107.151:4040\n",
      "\n",
      "log4j file: /home/jovyan/nfs-home/conf/pyspark-log4j-producer_app.properties\n",
      "\n",
      "driver log file: /home/jovyan/nfs-home/logs/pyspark-producer_app.log\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import socket\n",
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, length, when, col\n",
    "from pyspark.sql.types import BooleanType, IntegerType, LongType, StringType, ArrayType, FloatType, StructType, StructField\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.functions import PandasUDFType\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "\n",
    "\n",
    "# setting constants\n",
    "APP_NAME = \"producer_app\"\n",
    "NORMALIZED_APP_NAME = APP_NAME.replace('/', '_').replace(':', '_')\n",
    "\n",
    "APPS_TMP_DIR = os.path.join(os.getcwd(), \"tmp\")\n",
    "APPS_CONF_DIR = os.path.join(os.getcwd(), \"conf\")\n",
    "APPS_LOGS_DIR = os.path.join(os.getcwd(), \"logs\")\n",
    "LOG4J_PROP_FILE = os.path.join(APPS_CONF_DIR, \"pyspark-log4j-{}.properties\".format(NORMALIZED_APP_NAME))\n",
    "LOG_FILE = os.path.join(APPS_LOGS_DIR, 'pyspark-{}.log'.format(NORMALIZED_APP_NAME))\n",
    "EXTRA_JAVA_OPTIONS = \"-Dlog4j.configuration=file://{} -Dspark.hadoop.dfs.replication=1 -Dhttps.protocols=TLSv1.0,TLSv1.1,TLSv1.2,TLSv1.3\"\\\n",
    "    .format(LOG4J_PROP_FILE)\n",
    "\n",
    "LOCAL_IP = socket.gethostbyname(socket.gethostname())\n",
    "\n",
    "# preparing configuration files from templates\n",
    "for directory in [APPS_CONF_DIR, APPS_LOGS_DIR, APPS_TMP_DIR]:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "env = Environment(loader=FileSystemLoader('/opt'))\n",
    "template = env.get_template(\"pyspark_log4j.properties.template\")\n",
    "template\\\n",
    "    .stream(logfile=LOG_FILE)\\\n",
    "    .dump(LOG4J_PROP_FILE)\n",
    "\n",
    "# run spark\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(APP_NAME)\\\n",
    "    .master(\"k8s://https://10.32.7.103:6443\")\\\n",
    "    .config(\"spark.driver.host\", LOCAL_IP)\\\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\\\n",
    "    .config(\"spark.executor.instances\", \"2\")\\\n",
    "    .config(\"spark.executor.cores\", '3')\\\n",
    "    .config(\"spark.memory.fraction\", \"0.8\")\\\n",
    "    .config(\"spark.memory.storageFraction\", \"0.6\")\\\n",
    "    .config(\"spark.executor.memory\", '3g')\\\n",
    "    .config(\"spark.driver.memory\", \"3g\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"1g\")\\\n",
    "    .config(\"spark.kubernetes.memoryOverheadFactor\", \"0.3\")\\\n",
    "    .config(\"spark.driver.extraJavaOptions\", EXTRA_JAVA_OPTIONS)\\\n",
    "    .config(\"spark.kubernetes.namespace\", \"gkulagin-307618\")\\\n",
    "    .config(\"spark.kubernetes.driver.label.appname\", APP_NAME)\\\n",
    "    .config(\"spark.kubernetes.executor.label.appname\", APP_NAME)\\\n",
    "    .config(\"spark.kubernetes.container.image\", \"node03.st:5000/spark-executor:gkulagin-307618\")\\\n",
    "    .config(\"spark.local.dir\", \"/tmp/spark\")\\\n",
    "    .config(\"spark.driver.extraClassPath\", \"/home/jovyan/shared-data/my-project-name-jar-with-dependencies.jar\")\\\n",
    "    .config(\"spark.executor.extraClassPath\", \"/home/jovyan/shared-data/my-project-name-jar-with-dependencies.jar\")\\\n",
    "    .config(\"spark.kubernetes.executor.volumes.emptyDir.spark-local-dir-tmp-spark.mount.path\", \"/tmp/spark\")\\\n",
    "    .config(\"spark.kubernetes.executor.volumes.emptyDir.spark-local-dir-tmp-spark.mount.readOnly\", \"false\")\\\n",
    "    .config(\"spark.kubernetes.executor.volumes.hostPath.depdir.mount.path\", \"/home/jovyan/shared-data\")\\\n",
    "    .config(\"spark.kubernetes.executor.volumes.hostPath.depdir.options.path\", \"/nfs/shared\")\\\n",
    "    .config(\"spark.kubernetes.executor.volumes.hostPath.depdir.options.type\", \"Directory\")\\\n",
    "    .config(\"spark.kubernetes.executor.volumes.hostPath.depdir.mount.readOnly\", \"true\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# printing important urls and pathes\n",
    "print(\"Web UI: {}\".format(spark.sparkContext.uiWebUrl))\n",
    "print(\"\\nlog4j file: {}\".format(LOG4J_PROP_FILE))\n",
    "print(\"\\ndriver log file: {}\".format(LOG_FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robust-island",
   "metadata": {},
   "source": [
    "Posts must go to Kafka by a separate producer\n",
    "\n",
    "‚ÅÉ Divide the stream of posts into male and female\n",
    "\n",
    "‚ÅÉ Divide streams by age - up to 18, 18-27, 27 -40, 40-60, 60 and more\n",
    "\n",
    "‚ÅÉ Divide streams into words, filters stop words\n",
    "\n",
    "‚ÅÉ Counts words with a window of 1 hour / day / week in each stream (Which category more active writes?)\n",
    "\n",
    "‚ÅÉ Saves the result to Kafka and prints it to the console\n",
    "\n",
    "‚ÅÉ Processed results must be read by a separate consumer\n",
    "\n",
    "‚ÅÉ Use python, java or scala, DStreams or Structured Streaming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "honest-clause",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "enclosed-equipment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----------+\n",
      "|user_id|                text| timestamp|\n",
      "+-------+--------------------+----------+\n",
      "|  87449|–Ø –ª—é–±–ª—é –í–∞—Å. –Ø –≤—á...|1550165023|\n",
      "|  87449|call me by your n...|1553774858|\n",
      "|  87449|                  ü¶ã|1555602008|\n",
      "|  87449|         –ë—Ä–∞–≤–æ,–Æ—Ä–∞ !|1558105050|\n",
      "|  87449|                  üïä|1559301729|\n",
      "+-------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "posts_df = spark.read.json(\"/shared/bigdata20/followers_posts_api_final.json\")\\\n",
    "    .select(col(\"owner_id\").name('user_id'), \"text\", col(\"date\").alias(\"timestamp\"))\\\n",
    "    .where(\"text != ''\")\n",
    "posts_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "recognized-reform",
   "metadata": {},
   "outputs": [],
   "source": [
    "producer = KafkaProducer(bootstrap_servers=\"kafka-svc:9092\", value_serializer=str.encode)\n",
    "topic_name = \"posts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "saving-basketball",
   "metadata": {},
   "outputs": [],
   "source": [
    "located_df = posts_df.orderBy(\"timestamp\").rdd.toLocalIterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sustainable-rescue",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_post_cnt = posts_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "danish-store",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|‚ñã         | 13034/189848 [1:49:36<24:42:26,  1.99it/s]"
     ]
    }
   ],
   "source": [
    "for row in tqdm(located_df, total=user_post_cnt):\n",
    "    value = json.dumps(row.asDict(), ensure_ascii=False)\n",
    "    producer.send(topic_name, value)\n",
    "    time.sleep(0.5)\n",
    "\n",
    "print(\"Producer finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-market",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
